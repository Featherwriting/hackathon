"""
å°çº¢ä¹¦æ–‡ç« ä¸è¯„è®ºåˆ†æå·¥å…·
ç”¨äºæ£€ç´¢æ™¯ç‚¹/é¤å…ç›¸å…³æ–‡ç« ï¼Œæ±‡æ€»ç”¨æˆ·è¯„è®ºå¹¶ç”Ÿæˆç»¼åˆè¯„åˆ†æŠ¥å‘Š
"""

import json
import re
from typing import List, Dict, Optional
from pathlib import Path
from openai import OpenAI

# ====== é…ç½® ======
MODEL_NAME = "gpt-4o-mini"
client = OpenAI()

# å°çº¢ä¹¦æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯æ ¹æ®å®é™…è°ƒæ•´ï¼‰
NOTES_FILE = Path(__file__).parent / "data" / "search_contents_2025-11-16.json"
COMMENTS_FILE = Path(__file__).parent / "data" / "search_comments_2025-11-16.json"

# ç¼“å­˜æ•°æ®
_notes_cache: Optional[List[Dict]] = None
_comments_cache: Optional[Dict[str, List[Dict]]] = None


def _parse_chinese_number(num_str: str) -> int:
    """
    å°†ä¸­æ–‡æ•°å­—å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°
    ä¾‹: "4.2ä¸‡" -> 42000, "1356" -> 1356
    """
    if not num_str:
        return 0
    
    num_str = str(num_str).strip()
    
    try:
        # å¤„ç†"ä¸‡"å•ä½
        if 'ä¸‡' in num_str:
            base = float(num_str.replace('ä¸‡', ''))
            return int(base * 10000)
        # å¤„ç†"åƒ"å•ä½  
        elif 'åƒ' in num_str:
            base = float(num_str.replace('åƒ', ''))
            return int(base * 1000)
        # ç›´æ¥æ˜¯æ•°å­—
        else:
            return int(float(num_str))
    except (ValueError, AttributeError):
        return 0


def _load_notes() -> List[Dict]:
    """
    åŠ è½½å°çº¢ä¹¦æ–‡ç« åº“
    é¢„æœŸæ ¼å¼: [{"note_id": "...", "title": "...", "desc": "...", "nickname": "...", "liked_count": "...", ...}, ...]
    """
    global _notes_cache
    if _notes_cache is not None:
        return _notes_cache
    
    if not NOTES_FILE.exists():
        print(f"[XHS] Notes file not found: {NOTES_FILE}")
        return []
    
    try:
        with open(NOTES_FILE, 'r', encoding='utf-8') as f:
            _notes_cache = json.load(f)
        print(f"[XHS] Loaded {len(_notes_cache)} notes")
        return _notes_cache
    except Exception as e:
        print(f"[XHS] Error loading notes: {e}")
        return []


def _load_comments() -> Dict[str, List[Dict]]:
    """
    åŠ è½½å°çº¢ä¹¦è¯„è®ºåº“
    é¢„æœŸæ ¼å¼: [{"note_id": "...", "comment_id": "...", "content": "...", "like_count": "...", ...}, ...]
    è¿”å›: {note_id: [comments]}
    """
    global _comments_cache
    if _comments_cache is not None:
        return _comments_cache
    
    if not COMMENTS_FILE.exists():
        print(f"[XHS] Comments file not found: {COMMENTS_FILE}")
        return {}
    
    try:
        with open(COMMENTS_FILE, 'r', encoding='utf-8') as f:
            comments_list = json.load(f)
        
        # æŒ‰ note_id åˆ†ç»„
        comments_by_note: Dict[str, List[Dict]] = {}
        for comment in comments_list:
            note_id = comment.get("note_id")
            if note_id:
                if note_id not in comments_by_note:
                    comments_by_note[note_id] = []
                comments_by_note[note_id].append(comment)
        
        _comments_cache = comments_by_note
        print(f"[XHS] Loaded comments for {len(_comments_cache)} notes")
        return _comments_cache
    except Exception as e:
        print(f"[XHS] Error loading comments: {e}")
        return {}


def _search_relevant_notes(query: str, top_k: int = 10) -> List[Dict]:
    """
    æ ¹æ®æŸ¥è¯¢è¯ï¼ˆæ™¯ç‚¹/é¤å…åç§°ï¼‰æœç´¢ç›¸å…³æ–‡ç« 
    ç®€å•å®ç°ï¼šå…³é”®è¯åŒ¹é… + ç‚¹èµæ•°æ’åº
    """
    notes = _load_notes()
    if not notes:
        return []
    
    # å…³é”®è¯åŒ¹é…
    query_keywords = set(re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', query.lower()))
    
    scored_notes = []
    for note in notes:
        title = str(note.get("title", "")).lower()
        content = str(note.get("desc", "")).lower()  # å®é™…å­—æ®µæ˜¯ desc
        text = f"{title} {content}"
        
        # è®¡ç®—åŒ¹é…åº¦
        matches = sum(1 for kw in query_keywords if kw in text)
        if matches == 0:
            continue
        
        # ç»¼åˆè¯„åˆ†ï¼šåŒ¹é…åº¦ + ç‚¹èµæ•°
        likes = _parse_chinese_number(note.get("liked_count", "0"))  # å®é™…å­—æ®µæ˜¯ liked_count
        score = matches * 10 + likes * 0.01
        
        scored_notes.append({
            **note,
            "_relevance_score": score
        })
    
    # æŒ‰è¯„åˆ†æ’åº
    scored_notes.sort(key=lambda x: x["_relevance_score"], reverse=True)
    return scored_notes[:top_k]


def _aggregate_comments(note_ids: List[str]) -> List[Dict]:
    """
    èšåˆå¤šç¯‡æ–‡ç« çš„è¯„è®º
    è¿”å›: [{"note_id": "...", "content": "...", "likes": 0}, ...]
    """
    comments_by_note = _load_comments()
    
    all_comments = []
    for note_id in note_ids:
        if note_id in comments_by_note:
            all_comments.extend(comments_by_note[note_id])
    
    # æŒ‰ç‚¹èµæ•°æ’åºï¼Œå–é«˜èµè¯„è®º
    all_comments.sort(key=lambda x: _parse_chinese_number(x.get("like_count", "0")), reverse=True)
    return all_comments[:50]  # æœ€å¤šå–å‰50æ¡é«˜èµè¯„è®º


def _format_notes_for_llm(notes: List[Dict], comments: List[Dict]) -> str:
    """
    å°†æ–‡ç« å’Œè¯„è®ºæ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„ä¸Šä¸‹æ–‡
    """
    context = "ã€ç›¸å…³å°çº¢ä¹¦æ–‡ç« ã€‘\n\n"
    
    for i, note in enumerate(notes[:5], 1):  # æœ€å¤šå±•ç¤º5ç¯‡
        title = note.get("title", "æ— æ ‡é¢˜")
        content = note.get("desc", "")[:300]  # å®é™…å­—æ®µæ˜¯ descï¼Œæˆªå–å‰300å­—
        author = note.get("nickname", "åŒ¿å")  # å®é™…å­—æ®µæ˜¯ nickname
        likes = note.get("liked_count", "0")  # å®é™…å­—æ®µæ˜¯ liked_count
        note_id = note.get("note_id", "")
        
        context += f"æ–‡ç« {i}ï¼šã€Š{title}ã€‹\n"
        context += f"ä½œè€…ï¼š{author} | ç‚¹èµï¼š{likes}\n"
        context += f"å†…å®¹æ‘˜è¦ï¼š{content}...\n"
        context += f"note_id: {note_id}\n\n"
    
    context += "\nã€ç”¨æˆ·è¯„è®ºç²¾é€‰ã€‘\n\n"
    
    for i, comment in enumerate(comments[:20], 1):  # æœ€å¤šå±•ç¤º20æ¡è¯„è®º
        content = comment.get("content", "")
        likes = comment.get("like_count", "0")  # å®é™…å­—æ®µæ˜¯ like_count
        context += f"{i}. {content} (ğŸ‘ {likes})\n"
    
    return context


def analyze_xiaohongshu_media_score(spot_name: str, city: str = "") -> Dict:
    """
    åˆ†æå°çº¢ä¹¦ä¸Šå…³äºæŸä¸ªæ™¯ç‚¹/é¤å…çš„åª’ä½“è¯„åˆ†
    
    Args:
        spot_name: æ™¯ç‚¹æˆ–é¤å…åç§°
        city: åŸå¸‚åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæé«˜æœç´¢ç²¾åº¦ï¼‰
    
    Returns:
        {
            "success": bool,
            "spot_name": str,
            "summary": str,  # ç»¼åˆè¯„åˆ†æ€»ç»“
            "rating": float,  # 1-5 åˆ†
            "article_count": int,  # æ‰¾åˆ°çš„ç›¸å…³æ–‡ç« æ•°
            "comment_count": int,  # åˆ†æçš„è¯„è®ºæ•°
            "top_articles": [{"title": "...", "url": "...", "note_id": "..."}],
            "highlights": [str],  # äº®ç‚¹
            "concerns": [str],  # æ³¨æ„äº‹é¡¹
        }
    """
    print(f"[XHS Analyzer] Analyzing '{spot_name}' in '{city}'")
    
    # æ„å»ºæŸ¥è¯¢
    query = f"{city} {spot_name}" if city else spot_name
    
    # æœç´¢ç›¸å…³æ–‡ç« 
    relevant_notes = _search_relevant_notes(query, top_k=10)
    
    if not relevant_notes:
        return {
            "success": False,
            "spot_name": spot_name,
            "summary": f"æœªæ‰¾åˆ°å…³äºã€Œ{spot_name}ã€çš„å°çº¢ä¹¦æ–‡ç« ã€‚",
            "rating": 0.0,
            "article_count": 0,
            "comment_count": 0,
            "top_articles": [],
            "highlights": [],
            "concerns": []
        }
    
    # èšåˆè¯„è®º
    note_ids = [n["note_id"] for n in relevant_notes if "note_id" in n]
    comments = _aggregate_comments(note_ids)
    
    # æ ¼å¼åŒ–ä¸º LLM ä¸Šä¸‹æ–‡
    context = _format_notes_for_llm(relevant_notes, comments)
    
    # ä½¿ç”¨ LLM ç”Ÿæˆåˆ†ææŠ¥å‘Š
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…æ¸¸ä¸é¤é¥®è¯„ä»·åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å°çº¢ä¹¦æ–‡ç« å’Œç”¨æˆ·è¯„è®ºï¼Œç”Ÿæˆç»¼åˆè¯„åˆ†æŠ¥å‘Šã€‚

è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
- rating: ç»¼åˆè¯„åˆ†ï¼ˆ1-5åˆ†ï¼Œå°æ•°ï¼‰
- summary: ä¸€å¥è¯æ€»ç»“ï¼ˆ50å­—å†…ï¼‰
- highlights: 3-5ä¸ªäº®ç‚¹ï¼ˆæ•°ç»„ï¼Œæ¯ä¸ª10-20å­—ï¼‰
- concerns: 2-3ä¸ªæ³¨æ„äº‹é¡¹æˆ–ä¸è¶³ï¼ˆæ•°ç»„ï¼Œæ¯ä¸ª10-20å­—ï¼Œå¦‚æœæ²¡æœ‰è´Ÿé¢è¯„ä»·å¯ä¸ºç©ºï¼‰

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
    
    user_prompt = f"""ç›®æ ‡åœ°ç‚¹ï¼š{spot_name}

{context}

è¯·åŸºäºä»¥ä¸Šå°çº¢ä¹¦æ–‡ç« ä¸è¯„è®ºï¼Œç”Ÿæˆç»¼åˆè¯„åˆ†æŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰ã€‚"""
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            temperature=0.3,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        
        result_text = response.choices[0].message.content
        analysis = json.loads(result_text)
        
        # æå–çƒ­é—¨æ–‡ç« é“¾æ¥ï¼ˆä½¿ç”¨å®é™…çš„ note_url å­—æ®µï¼‰
        top_articles = []
        for note in relevant_notes[:3]:
            note_id = note.get("note_id", "")
            title = note.get("title", "æ— æ ‡é¢˜")
            # ä½¿ç”¨å®é™…çš„ note_url å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™æ„é€ 
            url = note.get("note_url", f"https://www.xiaohongshu.com/explore/{note_id}")
            top_articles.append({
                "title": title,
                "url": url,
                "note_id": note_id
            })
        
        return {
            "success": True,
            "spot_name": spot_name,
            "summary": analysis.get("summary", "ç»¼åˆè¯„ä»·è¾ƒå¥½"),
            "rating": float(analysis.get("rating", 4.0)),
            "article_count": len(relevant_notes),
            "comment_count": len(comments),
            "top_articles": top_articles,
            "highlights": analysis.get("highlights", []),
            "concerns": analysis.get("concerns", [])
        }
        
    except Exception as e:
        print(f"[XHS Analyzer] Error during LLM analysis: {e}")
        return {
            "success": False,
            "spot_name": spot_name,
            "summary": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
            "rating": 0.0,
            "article_count": len(relevant_notes),
            "comment_count": len(comments),
            "top_articles": [],
            "highlights": [],
            "concerns": []
        }


def format_analysis_for_user(analysis: Dict) -> str:
    """
    å°†åˆ†æç»“æœæ ¼å¼åŒ–ä¸ºç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯
    """
    if not analysis.get("success"):
        return analysis.get("summary", "åˆ†æå¤±è´¥")
    
    spot_name = analysis["spot_name"]
    rating = analysis["rating"]
    summary = analysis["summary"]
    highlights = analysis.get("highlights", [])
    concerns = analysis.get("concerns", [])
    articles = analysis.get("top_articles", [])
    
    # æ˜Ÿçº§æ˜¾ç¤º
    stars = "â­" * int(rating) + "â˜†" * (5 - int(rating))
    
    msg = f"""
ğŸ“± å°çº¢ä¹¦åª’ä½“è¯„åˆ†åˆ†æï¼š{spot_name}

{stars} {rating}/5.0 åˆ†

ğŸ“ ç»¼åˆè¯„ä»·ï¼š
{summary}

âœ¨ ç”¨æˆ·äº®ç‚¹ï¼š
"""
    
    for i, highlight in enumerate(highlights, 1):
        msg += f"{i}. {highlight}\n"
    
    if concerns:
        msg += "\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼š\n"
        for i, concern in enumerate(concerns, 1):
            msg += f"{i}. {concern}\n"
    
    if articles:
        msg += "\nğŸ”— å‚è€ƒæ–‡ç« ï¼š\n"
        for article in articles:
            title = article["title"]
            url = article["url"]
            msg += f"â€¢ {title}\n  {url}\n"
    
    msg += f"\nï¼ˆåŸºäº {analysis['article_count']} ç¯‡æ–‡ç« å’Œ {analysis['comment_count']} æ¡è¯„è®ºåˆ†æï¼‰"
    
    return msg
